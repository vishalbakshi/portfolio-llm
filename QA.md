## Background

This file contains questions and answers that will be provided in my final `llms_ctx.txt`. The first group of questions (context-aware) and third group (out-of-scope) were generated by Gemini in the same chat containing all previous chat messages on this portfolio-llm project. The second group of questions (cold chat) were generated by Gemini in a fresh chat without any context.

## Context-Aware QA

1. Explain your "Building Reliable Systems" philosophy. How did your work in data compliance inform this approach?
2. You've mentioned the "Narrow Fence, Long Leash" philosophy. How does your TinyScale Lab project represent a "Long Leash" for your "Narrow Fence" project, AgentFastbook?
3. In your "Finding My Moat" video, you talk about the importance of "boring" tasks. What is the most "boring" but critical part of the portfolio-llm project you're building?
4.  In your AgentFastbook video, you discovered that your manually created "answer components" were heavily influenced by the available context in the source text. How does this insight affect your strategy for the 'Retrieval' and 'Extraction' phases of the project?
5.  What was the most surprising failure mode you encountered when evaluating Haiku's text decomposition performance on your fastbook-benchmark?
6.  What is the core hypothesis of your TinyScale Lab project regarding the connection between training dynamics and model capabilities?
7.  You built several custom apps for evaluation. What was the most challenging technical aspect of building the LLM Judge Agreement App with FastHTML?
8.  In your debugging of LoRA models, you traced floating point errors back to the difference in matrix operations between merged and un-merged layers. Walk me through how you isolated that issue.
9.  You've done deep dives into both ColBERT and LLM-Foundry. What is a key design choice in one of those libraries that you find particularly elegant or effective?
10.  Based on your experiments with sequence packing, explain to a non-expert why simply enabling BinPackCollator in LLM-Foundry can lead to context contamination.
11.  (For a Researcher role) Your "Small-scale proxies" paper summary mentions using tiny models to predict instabilities in large models. How would you design an experiment to test this for a novel architecture?
12.  (For a Senior MLE role) You've explored full-precision vs. mixed-precision indexing in ColBERT. In a production environment with a tight budget, how would you decide which to use?
13.  You've identified that building evaluation infrastructure is a critical, often overlooked part of ML projects. How would you pitch the importance of allocating engineering time to a project like your "LM Scoring App" to a leadership team focused on shipping features?

## Cold Chat QA

14. Your fastbook-benchmark project is an excellent initiative for creating a high-quality evaluation dataset. Walk me through how you would productionize this system. How would you design a data pipeline that continuously and automatically ingests new information, runs your agent to generate question-answer pairs, and versions the dataset, ensuring reproducibility and reliability at 100x the scale?
15. You have a background in high-stakes compliance and a stated interest in reliable systems. Tell me about the most complex and subtle bug you've encountered in an ML system, perhaps in your TinyScale Lab or RAG projects. How did you methodically diagnose and fix it? What did this experience teach you about building more resilient ML systems from the start?
16. You've built your own evaluation framework for TinyScale Lab. In a team environment, we often have to decide whether to build a tool internally or use an existing third-party solution. When is it appropriate to build from scratch? Walk me through your decision-making process, using your evaluation framework as an example. What are the long-term maintenance and collaboration costs you'd consider?
17. In your AgentFastbook project, you're curating a clean, complex dataset. In a real-world product, the data is rarely this clean. Imagine you're tasked with building a RAG system for internal enterprise documents. What are the top three data quality challenges you would anticipate, and what specific strategies and tools would you employ to mitigate them before they impact model performance?
18. our interest in tiny models and resource-constrained research is fascinating. Let's say we've trained a large, powerful model for a specific task, but it's too slow and expensive for a real-time mobile application. What are the different families of techniques you would consider to create a 'tiny' version of this model? Please discuss the trade-offs between methods like distillation, quantization, and pruning
19. Youâ€™ve implemented many algorithms from scratch in your blog posts. Describe a time you had to work with a large, existing codebase you didn't create. How did you approach understanding the code, and how would you contribute a significant new feature (e.g., adding a new model architecture or data processing module) while adhering to existing design patterns and ensuring you don't break anything?
20. In your TinyScale Lab project, you're pre-training small language models from scratch. The prevailing wisdom is often that bigger is better. What is the core hypothesis you are exploring with this research? What specific, unanswered question about the relationship between training dynamics and model capabilities do you hope to answer?
21. Let's dive deeper into the custom evaluation framework for your TinyScale Lab. What makes it 'custom'? What phenomena can you measure with it that you couldn't with standard benchmarks like GLUE or HELM? How do you ensure your evaluation results are statistically significant and not just noise?
22. ou've published deep dives into SOTA libraries like ColBERT. Let's deconstruct it. From first principles, why does a 'bag-of-embeddings' approach like ColBERT's late interaction work so well for retrieval compared to dense vector search from a single CLS token? What are its fundamental limitations, and where do you see the next bottleneck in retrieval algorithms?
23. Your focus on resource-constrained research is timely. Besides pre-training smaller models, what other research areas do you believe are underexplored for creating powerful yet efficient AI? This could be in areas like data efficiency, novel architectures, or learning algorithms. Where would you focus your next research effort and why?
24. Research is full of dead ends. Tell me about a hypothesis in your TinyScale Lab or another project that turned out to be wrong. What was the hypothesis, how did your experiments invalidate it, and what did you learn from that null result? How did it change your research direction?
25. Imagine in three years, your line of research into tiny, reliable models is wildly successful. What does that future look like? What new capabilities will be unlocked, and what existing problems will be solved? Paint a picture of the tangible impact your work could have.
26. You're passionate about making reliable AI systems. Let's ground that in a real product. Imagine you're on a team building an AI-powered coding assistant. Who is the user you're most concerned about failing? Is it the junior developer, the senior architect, the DevOps engineer? How would a 'failure' in reliability manifest for that specific user, and how would that inform where you invest your technical efforts?
27. Let's say our team has a fixed budget for the next quarter. We can either invest in a project to improve our flagship model's accuracy by 5% on a key benchmark, or we can invest in a project to create a 'tiny' version of the model that runs on-device, albeit with 10% lower accuracy. As a technical stakeholder, how would you frame the pros and cons of these two paths for a non-technical product leader? What questions would you ask to help the team make the right strategic decision?
28. You've built extensive evaluation frameworks. In a product context, success is more than just academic benchmarks. For your TinyScale Lab project, if you were to turn that into a real product, what would be your 'North Star' metric? It can't be a technical metric like perplexity or accuracy alone. How would you measure whether your 'tiny models' are actually delivering value to users in the real world?

## Out-of-Scope QA

30. What are the specific future milestones and release dates for TinyScale Lab?
31. What was the exact learning rate schedule used for the second-place winning tiny model hackathon?
32. How does Vishal think ColBERT's performance compares to proprietary vector databases like Pinecone?
33. Can you provide the source code for the LLM Judge Agreement App?
34. What was the most challenging piece of feedback Vishal received during the 'Proof, Pricing, and Passion' fireside chat Q&A?
35. Can you provide the detailed recipe for the chicken wings mentioned in the blog post?
36. What is Vishal's opinion on the latest season of the show Severance?
37. Who is the current CEO of Microsoft?
38. What are the names of Vishal's family members?
39. Where did Vishal go to school?
40. List the names of the companies Vishal has worked at.
