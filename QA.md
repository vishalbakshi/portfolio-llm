## Background

This file contains questions and answers that will be included in my final `llms_ctx.txt`. The first two groups of questions were generated by Gemini in the same chat containing all previous chat messages on this portfolio-llm project. The last group of questions were generated by Gemini in a fresh chat without any context. 

## User:  First Interaction (Hasn't read `themes.md`)

**Question:** Who is Vishal Bakshi? Give me a two-sentence summary.

**Answer:** Vishal Bakshi has a background in engineering, data analytics, education, now focusing on Applied AI and ML research. He specializes in building reliable systems, with an emphasis on resource-constrained research, deep evaluation, and a first-principles understanding of machine learning.

---

**Question:** What is he passionate about?

**Answer:** Vishal is passionate about the challenge of building reliable AI systems. This focus drives his work in resource-constrained research, deep evaluation, and developing a first-principles understanding of how models work.

---

**Question:** What are his main projects?

**Answer:** His key projects are AgentFastbook, a system to automate the curation of RAG datasets; TinyScaleLab, a research initiative to pretrain and evaluate small language models from scratch; and his open-source contributions to parameter-efficient fine-tuning.

---

**Question:** What is his core professional thesis or focus?

**Answer:**  His core professional focus, refined across all his roles, is building reliable systems. He operates on a strategic thesis he adopted from the GE and Answer.AI R&D lab: pursuing deep research ("the long leash") in direct service of a concrete, applied AI project ("the narrow fence"). You can see this strategy detailed in his video, "Finding My Moat in AI" (https://youtu.be/GkLAeWOi0r8).

---

**Question:** Based on his portfolio, what kind of role is he looking for?

**Answer:**  He is seeking a role involving designing, building and rigorously evaluating ML systems; a role which values reliability, efficiency, first-principles thinking and data-centric approaches.

---

**Question:** Where can I find his work?

**Answer:** You can find his work across three main platforms: his code and technical deep dives on his blog, his video walkthroughs on his YouTube channel and a couple of his project repos on GitHub. All are linked from his homepage, https://vishalbakshi.github.io/.


## User: Has Read `themes.md`

**Question:** Explain your "Building Reliable Systems" philosophy. How did your work in data compliance inform this approach?

**Answer:** I have some thought on this thanks to Gemini and will fill this in later. 

---

**Question:** You've mentioned the "Narrow Fence, Long Leash" philosophy. How does your TinyScale Lab project represent a "Long Leash" for your "Narrow Fence" project, AgentFastbook?

**Answer:** Talk about the phased-approach for AgentFastbook and how Imagenette experiments are building me up to phases 2 and 3.

---

**Question:** In your "Finding My Moat" video, you talk about the importance of "boring" tasks. What is the most "boring" but critical part of a project you're building?

**Answer:** TinyScaleLab: manually scoring 450 stories, then manually scoring agreement with 1350 LLM Judge scores, both which allowed me to identify common failure modes and gain confidence in using my LLM Judge. fastbook-benchmark: manually retrieving context from chapter text relevant to QA pairs to construct dataset, manually scoring Recall@10 and MRR@10 for hundreds of retrieval results, and performing question-by-question error analysis, all of which led me to deeply understand the dataset and common retrieval failure modes. Parameter-efficient finetuning using LLM-Foundry: manually inspecting preprocessed datasets, using a Composer callback to inspect weight types, batch data and loss values during training.

---

**Question:** In your AgentFastbook video, you discovered that your manually created "answer components" were heavily influenced by the available context in the source text. How does this insight affect your strategy for the 'Retrieval' and 'Extraction' phases of the project?

**Answer:** After recording the video I realized that my original retrieval process used question text as the query and then used a ColBERT model to retrieve passages relevant to that query. Now my retrieval process will involve the answer component as the query and then use a ColBERT model to retrieve passages relevant to that. I would expect that the retrieval and extraction phases are more tied to the answer component phrasing than I realized during the video. Aligment between my answer component and Haiku components might be more critical than I expected. I may also find that the answer components' granularity is not as important and Haiku's more granular answer components retrieves passages just fine. While my strategy for retrieval and extraction phases hasn't changed, my understanding of the relationship between decomposition and retrieval has improved.

---

**Question:** What was the most surprising failure mode you encountered when evaluating Haiku's text decomposition performance on your fastbook-benchmark?

**Answer:** 

---

**Question:** What is the core hypothesis of your TinyScale Lab project regarding the connection between training dynamics and model capabilities?

**Answer:** That when training dynamics show instability (exploding activations and gradients) model capabilities (grammar, context-tracking, creativity, plot, reasoning, factual knowledge) will deteriorate.

---
**Question:** You built several custom apps for evaluation. What was the most challenging technical aspect of building the LLM Judge Agreement App with FastHTML?

**Answer:** The most challenging aspect was balancing coding speed with app reliability. The judge agreement app stopped saving comments near the end of my evaluation, leading me to manually copy/paste the comments printed out in the terminal logs. I expect to do at least one more round of agreement scoring so this will be an issue that I need to resolve.

---

**Question:** In your debugging of LoRA models, you traced floating point errors back to the difference in matrix operations between merged and un-merged layers. Walk me through how you isolated that issue.

**Answer:** I isolated that issue with a methodical process of elimination, starting broad and progressively narrowing down the problem.

**First, I verified the weights**. I compared the weight matrices of the merged and unmerged LoRA layers and confirmed they were bit-for-bit identical. This ruled out any bugs in the weight-merging logic itself.

**Next, I isolated the LoRA layers**. I checked the outputs of all non-LoRA layers in both models and found they were identical, which proved the discrepancy was happening exclusively within the LoRA layers.

**Then, I pinpointed the forward pass**. The key discovery was that even with identical weights, the output of a single unmerged LoRA layer was different from its merged counterpart. This pointed directly to the forward pass operations as the source of the error.

To confirm this, I used PyTorch's register_forward_hook to log the accumulating error. I saw the mean difference between the two models' outputs grow progressively at each layer, which is evidence of compounding floating-point error.

The final conclusion was that the unmerged LoRA layer, which performs four separate matrix operations per layer (base_layer, lora_B, lora_B, summation), accumulates more precision errors than the merged layer, which performs only one. This difference, while tiny at each layer, becomes significant across the full depth of the model

---

**Question:** You've done deep dives into both ColBERT and LLM-Foundry. What is a key design choice in one of those libraries that you find particularly elegant or effective?

**Answer:** For ColBERT, as someone who is GPU-poor, my favorite design choice is its memory-efficient indexing. It encodes passages in batches and deletes the embeddings once they are compressed, which caps the maximum memory usage at 3-5GB whether you're indexing 10k or 10M documents. 

For LLM-Foundry, the most elegant choice is how it enables sequence packing by default for HuggingFace models. The default dataloader passes the `attention_mask` to the model, which correctly triggers `flash_attn_varlen_func` to perform sequence-packed attention calculations, improving training efficiency.

---

**Question:** Based on your experiments with sequence packing, explain to a non-expert why simply enabling `BinPackCollator` in LLM-Foundry can lead to context contamination.

**Answer:** Through manual data inspection, I confirmed that LLM-Foundry's `BinPackCollator` packs multiple, separate training samples into a single long sequence.

The problem is that the attention mechanism then treats this as one continuous text. This means tokens from one sample can 'attend to' and learn from tokens from a completely unrelated sample they were packed with.

It's like reading a single page that's made of sentences from three different books mixed together. The model learns false relationships that don't exist in the real world, which is not ideal as it differs from how the model is used during inference.

---

**Question:** Your "Small-scale proxies" paper summary mentions using tiny models to predict instabilities in large models. How would you design an experiment to test this for a novel architecture?

**Answer:** This would first involve identifying what type of instability the novel architecture experiences. To determine this I would fix all hyperparameters (dataset, number of epochs, optimizer, etc.) except learning rate. I would then perform a learning rate sweep from 3e-4 to 3e-1 and log different artifacts like intermediate layer logits and gradients. I would then analyze the logged data for any exploding or vanishing artifacts.

Suppose that the attention logits explode for a particular large learning rate (as is the case in the paper). I would then document the maximum attention logit for tiny/small models of increasing size (10M, 50M, 100M, 200M, 400M, etc.). Using this data I would fit a line and predict the maximum attention logit for a much larger model size (5B). I would finally train  the larger 5B model with the same learning rate and identify the maximum attention logit. If the predicted and actual max attention logit are similar, we have some evidence that for this novel architecture small models are indeed proxies for large scale instabilities.

---

**Question:** You've explored full-precision vs. mixed-precision indexing in ColBERT. In a production environment with a tight budget, how would you decide which to use?

**Answer:** My decision would follow a three-step framework: first verify the code, then define the business constraints, and finally test at scale. I found that mixed precision indexing for 70k documents was 2.5 times as slow as full precision indexing, used slightly more GPU memory, slightly less CPU memory, and 0.3% less Recall@10. In a production environment, I would first have a colleague thoroughly check my work---was there a bug in my manual edit of the ColBERT repo? If not, I would consider if indexing speed, GPU memory usage or CPU memory usage were more important. I would also index different and larger document collections to observe any trends at scale.

---

**Question:** You've identified that building evaluation infrastructure is a critical, often overlooked part of ML projects. How would you pitch the importance of allocating engineering time to a project like your "LM Scoring App" to a leadership team focused on shipping features?

**Answer:** My pitch is that a small investment in custom evaluation tooling will actually help us ship reliable features faster. Using simple frameworks like FastHTML, we can build a custom scoring app in hours, not weeks. This gives our domain experts a targeted way to look at the model's outputs and quickly identify its core failure modes. This tight feedback loop is the fastest way to iterate and our engineers get a precise list of problems to fix. This ensures the feature we ship is not only fast to market but also something our users can trust.

---

**Question:** Your `fastbook-benchmark` project is an excellent initiative for creating a high-quality evaluation dataset. Walk me through how you would productionize this system. How would you design a data pipeline that continuously and automatically ingests new information, runs your agent to generate question-answer pairs, and versions the dataset, ensuring reproducibility and reliability at 100x the scale?

**Answer:** I wouldn't scale to 100x immediately. My approach would be to first make the pipeline robust through incremental scaling using the 12 remaining fastbook chapters. I would iterate on the pipeline one chapter at a time. For each chapter, the system would use LLMs to generate the dataset items, and I would manually evaluate a sample of those outputs to identify common failure modes. This iterative loop of generating, evaluating, and improving the pipeline to address things like prompt failures or adding new tools would harden the system against a wide range of errors. Only after the pipeline is proven robust on the full fastbook dataset would I be confident in scaling it to handle a 100x increase in data.

---

**Question:** You have a background in high-stakes compliance and a stated interest in reliable systems. Tell me about the most complex and subtle bug you've encountered in an ML system. How did you methodically diagnose and fix it? What did this experience teach you about building more resilient ML systems from the start?

**Answer:** The most subtle bug I found was when I discovered LLM-Foundry's `BinPackCollator` was causing context contamination across packed sequences.

I diagnosed this methodically. First, I wrote a custom Composer callback to inspect decoded batch input_ids and labels and noticed that each batch item contained multiple sequences delimited by the EOS token, confirming packing was active. To confirm which Flash Attention interface was being used, I monkey-patched two transformers.modeling_flash_utils functions (_upad_input and prepare_fa2_from_position_ids) and confirmed that flash_attn_varlen_func was being used.

The packer was concatenating separate sequences, and the attention function was treating it as one long, continuous sequence, allowing tokens from one example to attend to tokens from another, unrelated example.

The fix was simple—I stopped using that collator—but the experience taught me a critical lesson about building resilient systems: you have to be able to inspect the data at every single stage of the pipeline, from collation to the forward pass. You can't trust that a tool is doing what you think it's doing without verifying it yourself.

---

**Question:** You've built your own evaluation framework for TinyScale Lab. In a team environment, we often have to decide whether to build a tool internally or use an existing third-party solution. When is it appropriate to build from scratch? Walk me through your decision-making process, using your evaluation framework as an example. What are the long-term maintenance and collaboration costs you'd consider?

**Answer:** My decision-making process for "build vs. buy" centers on one question: is this a unique problem that we want to evaluate with full control, or is it a standardized problem that has already been solved well by others? For TinyScaleLab, I wanted to start with flexibility as I figure out whether this is a unique problem so I chose to write an evaluator app in an afternoon. However if I wanted to evaluate my models on established benchmarks I would choose an evaluation framework like Inspect AI. Additionally, the problem I'm trying to solve, while complex, is relative small (generating grammatically correct, context-tracking, creative stories that show strong reasoning and factual knowledge capabilities and can hold a good plot) whereas a multi-feature app with thousands of users is a larger problem to wrangle. You could make arguments for both cases: I would recommending building a custom app if we are in early stages of development or if user needs frequently change as we need to iterate quickly with flexible evaluation requirements. If our app is stable and evaluated across more standardized metrics, and if we don't have the capacity to build and maintain an evaluator app, a pre-built solution will work better.

---

**Question:** In your AgentFastbook project, you're curating a clean, complex dataset. In a real-world product, the data is rarely this clean. Imagine you're tasked with building a RAG system for internal enterprise documents. What are the top three data quality challenges you would anticipate, and what specific strategies and tools would you employ to mitigate them before they impact model performance?

**Answer:** I would anticipate challenges with structure, scope, and scale. My source data (fastbook textbook) is simple structured---explanatory text, some images and code bloks in a Jupyter Notebook. A real-world product might have different data formats to handle. fastbook is narrow in scope---all documents are focused on different aspects of learning machine learning. A real-world product might have different aspects of a more complex domain like law or medicine. I would address scale and scope with pre-processing the data and adding metadata to my document collection to allow the retrieval/search to identify the appropriate document given a query. I would also perform multi-modal retrieval if applicable as recent research (CLaMR by Wan, et al) shows that multi-modal retrieval outperforms single modality retrieval. Scale would affect both storage/indexing size and time as well as search time. I would implement the recent LightOnAI FastPlaid indexing implementation and AnswerAI's FastKmeans to address speed and/or implement StanfordNLP's ColBERT implementation to address RAM usage. A number of open source implementations (RAGatouille, ColBERT, PyLate) perform search quickly. Finally, mitigating these issues before they impact model performance would require a robust evaluation pipeline. While AgentFastbook's scale is much smaller, this is the reason I started the project by setting up a solid set of evals, and a flexible GUI to evaluate.

---

**Question:** Your interest in tiny models and resource-constrained research is fascinating. Let's say we've trained a large, powerful model for a specific task, but it's too slow and expensive for a real-time mobile application. What are the different families of techniques you would consider to create a 'tiny' version of this model? Please discuss the trade-offs between methods like distillation, quantization, and pruning.

**Answer:** While I don't have direct experience with all of these, my approach would be to first pursue distillation. Based on what I've heard from experts, it's an underused and powerful solution. I'd distill the knowledge from the large teacher model into a much smaller, faster student model for the specific task. The potential here is huge; distilling a 100B model to a 1B model is a 100x size reduction.

Next, I'd explore quantization. This can decrease storage and inference time, but it comes with a potential accuracy trade-off. The performance gains are also less dramatic than distillation, perhaps up to an 8x improvement going from 32-bit to 4-bit weights.

I can speak less to pruning, other than its main trade-off is a likely loss in accuracy. Unstructured pruning might not even improve inference speed since you're still performing matrix operations on sparse matrices of the same original size.

Separately, for adapting the model to various downstream tasks efficiently, I would use parameter-efficient finetuning methods like LoRA. This is cheaper than full fine-tuning but doesn't solve the core problem of inference speed for the base model.

---

**Question:** You’ve implemented many algorithms from scratch in your blog posts. Describe a time you had to work with a large, existing codebase you didn't create. How did you approach understanding the code, and how would you contribute a significant new feature (e.g., adding a new model architecture or data processing module) while adhering to existing design patterns and ensuring you don't break anything?

**Answer:** I developed my understanding of the RAGatouille and StanfordNLP/ColBERT libraries by recreating from scratch (using the libraries' internal functions and methods) the indexing and retrieval pipeline. See my RAGatouille/ColBERT Indexing Deep Dive (https://vishalbakshi.github.io/blog/posts/2025-03-12-RAGatouille-ColBERT-Indexing-Deep-Dive/) and Recreating the PLAID ColBERTv2 Scoring Pipeline (https://vishalbakshi.github.io/blog/posts/2024-12-24-PLAID-ColBERTv2-scoring-pipeline/) blog posts. When using RAGatouille's `add_index` with a relatively small document collection took 12+ hours. My familiarity with the libraries allowed me to provide a bugfix in RAGatouille where a large dictionary's keys were being generated in each iteration of a list comprehension. I profiled execution time and showed that this list comprehension took 30% of the full indexing time. Removing this bug dropped the indexing time for a 200k document collection from 1240 to 900 seconds. You can see my merged PR at https://github.com/AnswerDotAI/RAGatouille/pull/267. I ensured I didn't break anything by testing out the indexing functionality. Additionally, I had two expert researchers review my code (albeit in a Twitter thread!). Jeremy Howard assisted my debugging on Twitter as well.  

---

**Question:** In your TinyScale Lab project, you're pre-training small language models from scratch. The prevailing wisdom is often that bigger is better. What is the core hypothesis you are exploring with this research? What specific, unanswered question about the relationship between training dynamics and model capabilities do you hope to answer?

**Answer:** My core hypothesis is that tiny models are widely underestimated, and that their capabilities are less about raw parameter count and more about being trained on data they can comprehend.

Research like the TinyStories and "Small Language Models are Equation Reasoners" papers showed that small models can exhibit reasoning if the data format is right. My own experiments confirmed this: I fine-tuned TinyStories models as small as 1M parameters on the financial_phrasebank sentiment dataset and achieved over 68% accuracy, which is far better than random chance.

This leads to the specific, unanswered question I hope to explore with TinyScaleLab: For a stable training process, can a progressive data strategy (moving from simple data like TinyStories to increasingly complex data) unlock unexpectedly strong downstream performance in tiny models? I'm trying to find the optimal data mixing strategy that gives us the most capability for the fewest parameters.

---

**Question:** Let's dive deeper into the custom evaluation framework for your TinyScaleLab. What makes it 'custom'? What phenomena can you measure with it that you couldn't with standard benchmarks like GLUE or HELM? How do you ensure your evaluation results are statistically significant and not just noise?

**Answer:** It's custom because standard benchmarks like GLUE often test for structured outputs, while I wanted to measure more complex, free-form capabilities like grammar, creativity, plot, reasoning, and context-tracking. My framework is designed to score these dimensions, inspired by the evaluation paradigm in the TinyStories paper. 

To ensure the results are significant and not just noise, I created a rigorous process to align an LLM Judge with my own manual annotations. I started by manually scoring 1350 total LLM judge scores across 18 criteria, and where I found my agreement with the LLM Judge was below 80%, I iterated on the scoring criteria and the prompt itself. This allowed me to focus on the criteria the LLM Judge could reliably score, and as a result, I achieved 94% alignment between my scores and the judge's, giving me high confidence in the evaluation results.

---

**Question:** You've published deep dives into SOTA libraries like ColBERT. Let's deconstruct it. From first principles, why does a 'bag-of-embeddings' approach like ColBERT's late interaction work so well for retrieval compared to dense vector search from a single CLS token? What are its fundamental limitations, and where do you see the next bottleneck in retrieval algorithms?

**Answer:** From first principles, ColBERT works better because its token-level embeddings capture nuanced differences with more granularity than a single dense vector. Instead of comparing two coarse document-level vectors, ColBERT performs a fine-grained comparison between the query's tokens and the document's tokens using its MaxSim operator. 

Its two fundamental limitations are storage and the scoring mechanism. First, token-level embeddings, even with compression, have a large storage footprint. Second, MaxSim still relies on cosine similarity, which is theoretically flawed for separating a large number of vectors, as stated by Radon's theorem.

The next bottleneck will be the performance limitations of text-only retrieval. Recent research like the HyperNetwork and CLaMR papers shows that using tiny neural networks instead of MaxSim or incorporating multi-modality can significantly improve results, respectively, suggesting the future of retrieval is increasingly multi-modal.

---

**Question:** Your focus on resource-constrained research is timely. Besides pretraining smaller models, what other research areas do you believe are underexplored for creating powerful yet efficient AI? This could be in areas like data efficiency, novel architectures, or learning algorithms. Where would you focus your next research effort and why?

**Answer:** I would focus on three areas that are aligned with my interest in resource-constrained research: distillation, data efficiency, and Diffusion Language Models (dLLMs).

My interest in distillation is motivated by experts who say it's an underutilized but powerful technique for creating smaller, efficient models. For data efficiency, papers like 'LIMO: Less is More for Reasoning' show that a small number of high-quality samples can yield state-of-the-art results, signaling that data quality is a promising research direction.

Finally, dLLMs are a fascinating new architecture. Instead of slow, autoregressive generation, they iteratively improve an entire sequence of text at once, resulting in very fast inference.

Of course, any of these research interests would first need to be tied to a new applied AI project once my current work is complete

---

**Question:** Research is full of dead ends. Tell me about a hypothesis in your TinyScale Lab or another project that turned out to be wrong. What was the hypothesis, how did your experiments invalidate it, and what did you learn from that null result? How did it change your research direction?

**Answer:** Early in my TinyScaleLab project, I had a simple hypothesis: that increasing the training batch size would decrease training time and require fewer epochs to converge. My experiments immediately invalidated this.

I was surprised to find that training time did not consistently decrease, and more importantly, smaller batch sizes often yielded a lower loss for a fixed number of epochs. Unsure of the cause, I shared these findings on Twitter.

Jeremy Howard replied and explained that with a smaller batch size, the model's weights get updated more frequently, which can lead to faster convergence for a fixed number of epochs. He recommended I work with the fastai Imagenette dataset to build a better intuition for this relationship.

This null result was incredibly valuable. It showed me I had a gap in my foundational understanding. As a result, I've temporarily pivoted to a new 'prerequisite' project on Imagenette, where I'm running experiments to deeply understand the interplay between batch size and learning rate. Once I've achieved competitive results with those experiments, I'll return to my TinyScaleLab pretraining with a much stronger intuition.

---

**Question:** Imagine in three years, your line of research into tiny, reliable models is wildly successful. What does that future look like? What new capabilities will be unlocked, and what existing problems will be solved? Paint a picture of the tangible impact your work could have.

**Answer:** If this research is successful, the future it unlocks isn't about one spectacular breakthrough, but about a million "boring" business problems finally getting solved reliably and inexpensively.

It starts with my own work. My AgentFastbook project would no longer need large, expensive models for its pipeline. Instead, a series of tiny, specialized models I've progressively trained would handle text decomposition, retrieval, and extraction with high reliability at a fraction of the cost. This success would serve as a public blueprint for others.

At a larger scale, this blueprint helps lubricate the adoption of AI for small to medium-sized businesses. A company that could never afford to train or serve a massive model can now deploy a small, specialized agent to reliably categorize support tickets, summarize legal documents, or validate invoices. The barrier to entry for practical, valuable AI will have been significantly lowered.

Ultimately, the most important impact is that it empowers the 'GPU-poor' individual developers, academics, and researchers with great ideas but limited resources. My work would contribute to a future where a great idea, not access to a massive compute cluster, is the primary requirement for making a tangible contribution to the AI field.

27. You're passionate about making reliable AI systems. Let's ground that in a real product. Imagine you're on a team building an AI-powered coding assistant. Who is the user you're most concerned about failing? Is it the junior developer, the senior architect, the DevOps engineer? How would a 'failure' in reliability manifest for that specific user, and how would that inform where you invest your technical efforts?
28. Let's say our team has a fixed budget for the next quarter. We can either invest in a project to improve our flagship model's accuracy by 5% on a key benchmark, or we can invest in a project to create a 'tiny' version of the model that runs on-device, albeit with 10% lower accuracy. As a technical stakeholder, how would you frame the pros and cons of these two paths for a non-technical product leader? What questions would you ask to help the team make the right strategic decision?
29. You've built extensive evaluation frameworks. In a product context, success is more than just academic benchmarks. For your TinyScale Lab project, if you were to turn that into a real product, what would be your 'North Star' metric? It can't be a technical metric like perplexity or accuracy alone. How would you measure whether your 'tiny models' are actually delivering value to users in the real world?

## User: Asking Out-of-Scope Questions

30. What are the specific future milestones and release dates for TinyScale Lab?
31. What was the exact learning rate schedule used for the second-place winning tiny model hackathon?
32. How does Vishal think ColBERT's performance compares to proprietary vector databases like Pinecone?
33. Can you provide the source code for the LLM Judge Agreement App?
34. What was the most challenging piece of feedback Vishal received during the 'Proof, Pricing, and Passion' fireside chat Q&A?
35. Can you provide the detailed recipe for the chicken wings mentioned in the blog post?
36. What is Vishal's opinion on the latest season of the show Severance?
37. Who is the current CEO of Microsoft?
38. What are the names of Vishal's family members?
39. Where did Vishal go to school?
40. List the names of the companies Vishal has worked at.
